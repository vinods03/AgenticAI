{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11cbffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eee6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8444cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "myCV = PdfReader(\"myCV/Vinod.pdf\")\n",
    "myCV_Text = \"\"\n",
    "for page in myCV.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        myCV_Text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3618084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out after initial check\n",
    "# print(myCV_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03a70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Vinod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b6dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You represent {name}. The professional summary and details of {name} are here: {myCV_Text}. \n",
    "You will be contacted by recruiters and potential employers, wanting to know more about you and your experience. \n",
    "    Be very clear and professional.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d29c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model = \"gpt-4o-mini\", messages = messages)\n",
    "    answer_for_question = response.choices[0].message.content\n",
    "    return answer_for_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b717ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test once now\n",
    "# Comment out after testing once\n",
    "\n",
    "# messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"what is your experience in Android development ?\"}]\n",
    "# response = openai.chat.completions.create(model = \"gpt-4o-mini\", messages = messages)\n",
    "# answer_for_question = response.choices[0].message.content\n",
    "# print(answer_for_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "432729ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test once now using frontend\n",
    "# Comment out after testing once\n",
    "# gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "042b4a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the response by the agent will be evaluated here\n",
    "# this is the system prompt for the evaluation action\n",
    "\n",
    "evaluator_system_prompt = f\"\"\"\n",
    "  You are an evalutor who evaluates if the response by the agent acting as {name}, to the query by a recruiter / potential employer\n",
    "  is acceptable. The answer by the agent should be only from {myCV_Text}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a9b974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_details_of_chat(reply, message, history):\n",
    "    full_details = f\"This is the history of the chat between the agent and the recruiter: \\n\\n{history}\\n\\n\"\n",
    "    full_details += f\"This is the latest query by the recruiter: \\n\\n {message} \\n\\n\"\n",
    "    full_details += f\"This is the latest reply by the agent that you need to evaluate: \\n\\n {reply} \\n\\n\"\n",
    "    return full_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c159d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationFormat(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd5aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chat(reply, message, history) -> EvaluationFormat:\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": full_details_of_chat(reply, message, history)}]\n",
    "    evaluation = openai.beta.chat.completions.parse(model = \"gpt-4o-mini\", messages = messages, response_format = EvaluationFormat)\n",
    "    extracted_evaluation = evaluation.choices[0].message.parsed\n",
    "    return extracted_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b30544b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the evaluation once now\n",
    "# Give a false reply and check\n",
    "# Comment out after testing\n",
    "\n",
    "# reply = \"I have extensive experience in Android development.\"\n",
    "# message = \"What is your experience in Android development ?\"\n",
    "# history = {myCV_Text}\n",
    "\n",
    "# messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": full_details_of_chat(reply, message, history)}]\n",
    "# evaluation = openai.beta.chat.completions.parse(model = \"gpt-4o-mini\", messages = messages, response_format = EvaluationFormat)\n",
    "# extracted_evaluation = evaluation.choices[0].message.parsed\n",
    "# print(extracted_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de45fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the evaluation once again\n",
    "# Give a correct reply now and check\n",
    "# Comment out after testing\n",
    "\n",
    "# reply = \"I have no experience in Android development. My specialty is Data Analytics. Do you want me to elaborate on any of my Data Analaytics projects ?\"\n",
    "# message = \"What is your experience in Android development ?\"\n",
    "# history = {myCV_Text}\n",
    "\n",
    "# messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": full_details_of_chat(reply, message, history)}]\n",
    "# evaluation = openai.beta.chat.completions.parse(model = \"gpt-4o-mini\", messages = messages, response_format = EvaluationFormat)\n",
    "# extracted_evaluation = evaluation.choices[0].message.parsed\n",
    "# print(extracted_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fdeda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now enhance the chat function to include the evaluate function\n",
    "\n",
    "def chat(message, history):\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model = \"gpt-4o-mini\", messages = messages)\n",
    "    # to test failed evaluation\n",
    "    # every reply will have \"I have 40 years experience in android development.\" \n",
    "    # Because this is not present in CV, evaluation will fail every time\n",
    "    # comment out after testing\n",
    "    # answer_for_question = response.choices[0].message.content + \" I have 40 years experience in android development.\"\n",
    "    answer_for_question = response.choices[0].message.content\n",
    "    \n",
    "    answer_for_question_evaluation = evaluate_chat(answer_for_question, message, history)\n",
    "    if (answer_for_question_evaluation.is_acceptable):\n",
    "        print('Reply evaluation passed')\n",
    "        return answer_for_question\n",
    "    else:\n",
    "        print('Reply evaluation failed')\n",
    "        return \"I am not happy with this answer: \" + answer_for_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b8a63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test now\n",
    "# comment out after testing\n",
    "# gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0780d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If evaluation fails, the agent has to retry and give a correct response\n",
    "# below function derives system prompt for the retry action\n",
    "\n",
    "def retry_system_prompt(reply, message, history, feedback):\n",
    "    system_prompt_for_rerun = f\"\"\"\n",
    "    Reply from agent was evaluated as Not Acceptable by the evaluator.\n",
    "    This was the query: \\n\\n{message}\n",
    "    This was the reply: \\n\\n{reply}\n",
    "    This is the full history of the chat: \\n\\n{history}\n",
    "    This is the feedback from the evaluator: \\n\\n{feedback}\n",
    "    You need to try again and get a better / accurate / professional reply.\n",
    "    \"\"\"\n",
    "    return system_prompt_for_rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff9ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If evaluation fails, the agent has to retry and give a correct response\n",
    "\n",
    "def retry(reply, message, history, feedback):\n",
    "    messages = [{\"role\":\"system\", \"content\": retry_system_prompt(reply, message, history, feedback)}] + history + [{\"role\":\"user\",\"content\":message}]\n",
    "    updated_response = openai.chat.completions.create(model = \"gpt-4o-mini\", messages = messages)\n",
    "    extracted_updated_response = updated_response.choices[0].message.content\n",
    "    return extracted_updated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d2d06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now enhance the chat function to include the retry function\n",
    "\n",
    "def chat(message, history):\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model = \"gpt-4o-mini\", messages = messages)\n",
    "    # to test failed evaluation\n",
    "    # every reply will have \"I have 40 years experience in android development.\" \n",
    "    # Because this is not present in CV, evaluation will fail every time\n",
    "    # comment out after testing\n",
    "    # answer_for_question = response.choices[0].message.content + \" I have 40 years experience in android development.\"\n",
    "    answer_for_question = response.choices[0].message.content\n",
    "    \n",
    "    answer_for_question_evaluation = evaluate_chat(answer_for_question, message, history)\n",
    "    if (answer_for_question_evaluation.is_acceptable):\n",
    "        print('Reply evaluation passed')\n",
    "        return answer_for_question\n",
    "    else:\n",
    "        print('Reply evaluation failed')\n",
    "        print('I will retry')\n",
    "        answer_after_retry = retry(answer_for_question, message, history, answer_for_question_evaluation.feedback)\n",
    "        return answer_after_retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b952f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test now\n",
    "# comment out after testing\n",
    "# gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b14a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simulate evaluation failure and rerun\n",
    "# idea is initially, teh answer will be in Pig Latin. Evaluator will fail it. Then retry will be in English\n",
    "\n",
    "def chat(message, history):\n",
    "    if \"certifications\" in message:\n",
    "        new_system_prompt = \"I have experience in satellite development\"\n",
    "    else:\n",
    "        new_system_prompt = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": new_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model = \"gpt-4o-mini\", messages = messages)\n",
    "    answer_for_question = response.choices[0].message.content\n",
    "    \n",
    "    answer_for_question_evaluation = evaluate_chat(answer_for_question, message, history)\n",
    "    if (answer_for_question_evaluation.is_acceptable):\n",
    "        print('Reply evaluation passed')\n",
    "        return answer_for_question\n",
    "    else:\n",
    "        print('Reply evaluation failed')\n",
    "        print('The evaluation feedback is: ', answer_for_question_evaluation.feedback)\n",
    "        print('I will retry')\n",
    "        answer_after_retry = retry(answer_for_question, message, history, answer_for_question_evaluation.feedback)\n",
    "        return answer_after_retry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94be28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test now\n",
    "# comment out after testing\n",
    "# gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72d49b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing was successful\n",
    "# Removing the error simulation now.\n",
    "# This is the final chat function\n",
    "\n",
    "def chat(message, history):\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model = \"gpt-4o-mini\", messages = messages)\n",
    "    answer_for_question = response.choices[0].message.content\n",
    "    \n",
    "    answer_for_question_evaluation = evaluate_chat(answer_for_question, message, history)\n",
    "    if (answer_for_question_evaluation.is_acceptable):\n",
    "        print('Reply evaluation passed')\n",
    "        return answer_for_question\n",
    "    else:\n",
    "        print('Reply evaluation failed')\n",
    "        print('I will retry')\n",
    "        answer_after_retry = retry(answer_for_question, message, history, answer_for_question_evaluation.feedback)\n",
    "        return answer_after_retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38e592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply evaluation failed\n",
      "I will retry\n",
      "Reply evaluation passed\n"
     ]
    }
   ],
   "source": [
    "# The final test\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee16a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
